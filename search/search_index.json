{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Entrainement","text":""},{"location":"#entrainement","title":"Entrainement","text":""},{"location":"#1-personnalisation-du-dataset","title":"1. Personnalisation du  dataSet","text":""},{"location":"#11-que-fait-elle","title":"1.1 Que fait-elle ?","text":"<p>Chargement, Pr\u00e9traitement et Construction des \u00e9chantillons de donn\u00e9es \u00e0 partir de fichiers de maillage 3D au format VTK.  </p>"},{"location":"#12-quelles-sont-les-etapes","title":"1.2 Quelles sont les \u00e9tapes ?","text":"<ul> <li>Chargement d'un maillage 3D \u00e0 partir d'un fichier VTK</li> </ul> <pre><code>     mesh = load(i_mesh)\n\n</code></pre> <ul> <li>Extraction des \u00e9tiquettes \u00e0 partir des donn\u00e9es du maillage.   </li> </ul> <pre><code>    labels = mesh.celldata['MaterialIds'].astype('int32').reshape(-1, 1)\n</code></pre> <ul> <li>Effectue diverses op\u00e9rations de pr\u00e9traitement sur le maillage, y compris le calcul des normales, la normalisation des donn\u00e9es   </li> </ul> <pre><code>    points = mesh.points()\n    mean_cell_centers = mesh.center_of_mass()\n    points[:, 0:3] -= mean_cell_centers[0:3]\n    ids = np.array(mesh.faces())\n    cells = points[ids].reshape(mesh.ncells, 9).astype(dtype='float32')\n    mesh.compute_normals()\n    normals = mesh.celldata['Normals']\n    barycenters = mesh.cell_centers() \n    barycenters -= mean_cell_centers[0:3]\n    maxs = points.max(axis=0)\n    mins = points.min(axis=0)\n    means = points.mean(axis=0)\n    stds = points.std(axis=0)\n    nmeans = normals.mean(axis=0)\n    nstds = normals.std(axis=0)\n    for i in range(3):\n        cells[:, i] = (cells[:, i] - means[i]) / stds[i] \n        cells[:, i+3] = (cells[:, i+3] - means[i]) / stds[i]\n        cells[:, i+6] = (cells[:, i+6] - means[i]) / stds[i]\n        barycenters[:,i] = (barycenters[:,i] - mins[i]) / (maxs[i]-mins[i])\n        normals[:,i] = (normals[:,i] - nmeans[i]) / nstds[i]\n        X = np.column_stack((cells, barycenters, normals))\n        Y = labels\n        X_train = np.zeros([self.patch_size, X.shape[1]], dtype='float32')\n        Y_train = np.zeros([self.patch_size, Y.shape[1]], dtype='int32')\n        S1 = np.zeros([self.patch_size, self.patch_size], dtype='float32')\n        S2 = np.zeros([self.patch_size, self.patch_size], dtype='float32')```\n</code></pre> <ul> <li>S\u00e9lection d'un sous-ensemble d'\u00e9chantillons positifs (dents) et z\u00e9ro (gencive) en fonction du nombre de patch_size.   </li> </ul> <pre><code>    positive_idx = np.argwhere(labels&gt;0)[:, 0] \n    negative_idx = np.argwhere(labels==0)[:, 0] \n</code></pre> <ul> <li>Calcul d'une matrice de distance entre les \u00e9chantillons s\u00e9lectionn\u00e9s.  </li> </ul> <pre><code>    num_positive = len(positive_idx)\n    if num_positive &gt; self.patch_size:\n        positive_selected_idx = np.random.choice(positive_idx, \n        size=self.patch_size, replace=False)\n        selected_idx = positive_selected_idx\n    else:   \n        num_negative = self.patch_size - num_positive \n        positive_selected_idx = np.random.choice(positive_idx, \n        size=num_positive, replace=False)\n        negative_selected_idx = np.random.choice(negative_idx, \n        size=num_negative, replace=False)\n        selected_idx = np.concatenate((positive_selected_idx, \n        negative_selected_idx))\n    selected_idx = np.sort(selected_idx, axis=None)\n    X_train[:] = X[selected_idx, :]\n    Y_train[:] = Y[selected_idx, :]\n    if  torch.cuda.is_available():\n        TX = torch.as_tensor(X_train[:, 9:12], device='cuda')\n        TD = torch.cdist(TX, TX)\n        D = TD.cpu().numpy()\n    else:\n        D = distance_matrix(X_train[:, 9:12], X_train[:, 9:12])\n</code></pre> <ul> <li>Cr\u00e9ation des matrices de similarit\u00e9 S1 et S2 bas\u00e9es sur la distance.   </li> </ul> <pre><code>    S1[D&lt;0.1] = 1.0\n    S1 = S1 / np.dot(np.sum(S1, axis=1, keepdims=True), \n    np.ones((1, self.patch_size)))\n    S2[D&lt;0.2] = 1.0\n    S2 = S2 / np.dot(np.sum(S2, axis=1, keepdims=True), \n    np.ones((1, self.patch_size)))\n    X_train = X_train.transpose(1, 0)\n    Y_train = Y_train.transpose(1, 0)\n</code></pre> <ul> <li>Production d'un \u00e9chantillon sous forme de dictionnaire PyTorch contenant les donn\u00e9es de maillage (cells), les \u00e9tiquettes (labels),ainsi que les matrices de similarit\u00e9 (A_S et A_L).  </li> </ul> <pre><code>    sample = {'cells': torch.from_numpy(X_train), \n              'labels': torch.from_numpy(Y_train),\n              'A_S': torch.from_numpy(S1), 'A_L': torch.from_numpy(S2)}\n</code></pre>"},{"location":"#13-cest-quoi-lutilite-de-ces-donnees","title":"1.3 C'est quoi l'utilit\u00e9 de ces donn\u00e9es ?","text":"<p>En pr\u00e9vision de l'utilisation d'un mod\u00e8le de segmentation de maillages 3D appel\u00e9 MeshSegNet, on envisage de travailler avec des caract\u00e9ristiques sp\u00e9cifiques des maillages 3D, \u00e0 savoir les \"cells\" (repr\u00e9sentant les propri\u00e9t\u00e9s 3D d'un maillage) ainsi que les matrices de similarit\u00e9 \"A_S\" et \"A_L\", et les \u00e9tiquettes (labels) associ\u00e9es \u00e0 un \u00e9chantillon pendant la phase d'entra\u00eenement. L'objectif de cette approche est d'exploiter les informations contenues dans les \"cells\" et les matrices de similarit\u00e9 pour pr\u00e9dire de mani\u00e8re supervis\u00e9e les \u00e9tiquettes, ce qui est essentiel pour la t\u00e2che de segmentation effectu\u00e9e  par le mod\u00e8le MeshSegNet.</p>"},{"location":"#14-quel-est-lobjectif","title":"1.4 Quel est l'objectif ?","text":"<ul> <li>Pr\u00e9paration des donn\u00e9es pour l'apprentissage automatique, o\u00f9 les maillages 3D repr\u00e9sentent des objets g\u00e9om\u00e9triques.   </li> <li>Extraction des \u00e9tiquettes  des donn\u00e9es du maillage pour la classification ou la segmentation.  </li> </ul>"},{"location":"#2-definition-de-larchitecture-meshsegnet","title":"2. D\u00e9finition de l\u2019architecture MeshSegNet","text":""},{"location":"#21-quest-ce-quil-presente","title":"2.1 Qu'est ce qu'il pr\u00e9sente ?","text":"<p>Une d\u00e9finition d'une architecture de r\u00e9seau de neurones appel\u00e9e MeshSegNet pour effectuer la segmentation de maillages 3D.  </p>"},{"location":"#22-quelle-est-la-composition-de-cette-architecture","title":"2.2 Quelle est la composition de cette architecture ?","text":"<p>Une s\u00e9rie de couches de convolution, de normalisation et d'op\u00e9rations non lin\u00e9aires pour apprendre des repr\u00e9sentations significatives \u00e0 partir des donn\u00e9es de maillage.  </p>"},{"location":"#23-quel-enchainement-logique-entre-ces-couches","title":"2.3 Quel encha\u00eenement logique entre ces couches ?","text":"<ul> <li>Commen\u00e7ant avec la STN3d (Spatial Transformer Network 3D) qui effectue des transformations spatiales sur les donn\u00e9es en entr\u00e9e pour aider \u00e0 l'alignement des maillages. Ensuite, il passe par plusieurs couches de convolution, chacune suivie d'une normalisation par lots (BatchNorm1d) et d'une fonction d'activation ReLU. Ces couches extraient des caract\u00e9ristiques pertinentes des donn\u00e9es (exploration des donn\u00e9es).   </li> </ul> <pre><code>    class STN3d(nn.Module):\n        def __init__(self, channel):\n            super(STN3d, self).__init__()\n            self.conv1 = torch.nn.Conv1d(channel, 64, 1)\n            self.conv2 = torch.nn.Conv1d(64, 128, 1)\n            self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n            self.fc1 = nn.Linear(1024, 512)\n            self.fc2 = nn.Linear(512, 256)\n            self.fc3 = nn.Linear(256, 9)\n            self.relu = nn.ReLU()\n            self.bn1 = nn.BatchNorm1d(64)\n            self.bn2 = nn.BatchNorm1d(128)\n            self.bn3 = nn.BatchNorm1d(1024)\n            self.bn4 = nn.BatchNorm1d(512)\n            self.bn5 = nn.BatchNorm1d(256)\n        def forward(self, x):\n            batchsize = x.size()[0]\n            x = F.relu(self.bn1(self.conv1(x)))\n            x = F.relu(self.bn2(self.conv2(x)))\n            x = F.relu(self.bn3(self.conv3(x)))\n            x = torch.max(x, 2, keepdim=True)[0]\n            x = x.view(-1, 1024)\n            x = F.relu(self.bn4(self.fc1(x)))\n            x = F.relu(self.bn5(self.fc2(x)))\n            x = self.fc3(x)\n            iden = Variable(torch.from_numpy\n            (np.array([1, 0, 0, 0, 1, 0, 0, 0, 1]).\n            astype(np.float32))).view(1, 9).repeat(\n                batchsize, 1)\n            if x.is_cuda:\n                iden = iden.to(x.get_device())\n            x = x + iden\n            x = x.view(-1, 3, 3)\n            return x\n</code></pre> <ul> <li>Le mod\u00e8le comprend \u00e9galement un module appel\u00e9 STNkd qui est similaire \u00e0 STN3d mais adapt\u00e9 \u00e0 une dimension k quelconque.    </li> </ul> <pre><code>class STNkd(nn.Module):\n    def __init__(self, k=64):\n        super(STNkd, self).__init__()\n        self.conv1 = torch.nn.Conv1d(k, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 512, 1)\n        self.fc1 = nn.Linear(512, 256)\n        self.fc2 = nn.Linear(256, 128)\n        self.fc3 = nn.Linear(128, k * k)\n        self.relu = nn.ReLU()\n        self.bn1 = nn.BatchNorm1d(64)\n        self.bn2 = nn.BatchNorm1d(128)\n        self.bn3 = nn.BatchNorm1d(512)\n        self.bn4 = nn.BatchNorm1d(256)\n        self.bn5 = nn.BatchNorm1d(128)\n        self.k = k\n    def forward(self, x):\n        batchsize = x.size()[0]\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = torch.max(x, 2, keepdim=True)[0]\n        x = x.view(-1, 512)\n        x = F.relu(self.bn4(self.fc1(x)))\n        x = F.relu(self.bn5(self.fc2(x)))\n        x = self.fc3(x)\n        iden = Variable(torch.from_numpy(np.eye(self.k).flatten().\n        astype(np.float32))).view(1, self.k * self.k).repeat(batchsize, 1)\n        if x.is_cuda:\n            iden = iden.to(x.get_device())\n        x = x + iden\n        x = x.view(-1, self.k, self.k)\n        return x\n</code></pre> <ul> <li>Le processus commence par l'extraction de caract\u00e9ristiques par les couches de convolution, suivi de l'application de MLP-1 ou MLP-2.Cette \u00e9tape est cruciale pour am\u00e9liorer la capacit\u00e9 du mod\u00e8le \u00e0 effectuer une segmentation pr\u00e9cise, car elle permet d'apprendre des caract\u00e9ristiques distinctives tout en capturant des relations complexes entre les donn\u00e9es. Ensuite, les caract\u00e9ristiques ainsi obtenues sont combin\u00e9es avec les matrices de similarit\u00e9 \u00e0 l'aide des couches GLM-1 ou GLM-2. Enfin, pour int\u00e9grer diff\u00e9rentes informations de mani\u00e8re coh\u00e9rente, le mod\u00e8le utilise des couches de fusion et d'upsampling. Ce processus global vise \u00e0 cr\u00e9er une repr\u00e9sentation riche et discriminante des donn\u00e9es 3D tout en exploitant les informations de similarit\u00e9 pour la t\u00e2che de segmentation.   </li> </ul> <pre><code>class MeshSegNet(nn.Module):\n    def __init__(self, num_classes=15, num_channels=15, \n                 with_dropout=True, dropout_p=0.5):\n        super(MeshSegNet, self).__init__()\n        self.num_classes = num_classes\n        self.num_channels = num_channels\n        self.with_dropout = with_dropout\n        self.dropout_p = dropout_p\n\n        # MLP-1 [64, 64]\n        self.mlp1_conv1 = torch.nn.Conv1d(self.num_channels, 64, 1)\n        self.mlp1_conv2 = torch.nn.Conv1d(64, 64, 1)\n        self.mlp1_bn1 = nn.BatchNorm1d(64)\n        self.mlp1_bn2 = nn.BatchNorm1d(64)\n        # FTM (feature-transformer module)\n        self.fstn = STNkd(k=64)\n        # GLM-1 (graph-contrained learning modulus)\n        self.glm1_conv1_1 = torch.nn.Conv1d(64, 32, 1)\n        self.glm1_conv1_2 = torch.nn.Conv1d(64, 32, 1)\n        self.glm1_bn1_1 = nn.BatchNorm1d(32)\n        self.glm1_bn1_2 = nn.BatchNorm1d(32)\n        self.glm1_conv2 = torch.nn.Conv1d(32+32, 64, 1)\n        self.glm1_bn2 = nn.BatchNorm1d(64)\n        # MLP-2\n        self.mlp2_conv1 = torch.nn.Conv1d(64, 64, 1)\n        self.mlp2_bn1 = nn.BatchNorm1d(64)\n        self.mlp2_conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.mlp2_bn2 = nn.BatchNorm1d(128)\n        self.mlp2_conv3 = torch.nn.Conv1d(128, 512, 1)\n        self.mlp2_bn3 = nn.BatchNorm1d(512)\n        # GLM-2 (graph-contrained learning modulus)\n        self.glm2_conv1_1 = torch.nn.Conv1d(512, 128, 1)\n        self.glm2_conv1_2 = torch.nn.Conv1d(512, 128, 1)\n        self.glm2_conv1_3 = torch.nn.Conv1d(512, 128, 1)\n        self.glm2_bn1_1 = nn.BatchNorm1d(128)\n        self.glm2_bn1_2 = nn.BatchNorm1d(128)\n        self.glm2_bn1_3 = nn.BatchNorm1d(128)\n        self.glm2_conv2 = torch.nn.Conv1d(128*3, 512, 1)\n        self.glm2_bn2 = nn.BatchNorm1d(512)\n        # MLP-3\n        self.mlp3_conv1 = torch.nn.Conv1d(64+512+512+512, 256, 1)\n        self.mlp3_conv2 = torch.nn.Conv1d(256, 256, 1)\n        self.mlp3_bn1_1 = nn.BatchNorm1d(256)\n        self.mlp3_bn1_2 = nn.BatchNorm1d(256)\n        self.mlp3_conv3 = torch.nn.Conv1d(256, 128, 1)\n        self.mlp3_conv4 = torch.nn.Conv1d(128, 128, 1)\n        self.mlp3_bn2_1 = nn.BatchNorm1d(128)\n        self.mlp3_bn2_2 = nn.BatchNorm1d(128)\n        # output\n        self.output_conv = torch.nn.Conv1d(128, self.num_classes, 1)\n        if self.with_dropout:\n            self.dropout = nn.Dropout(p=self.dropout_p)\n\n</code></pre>"},{"location":"#24-quelle-utilite","title":"2.4 Quelle utilit\u00e9 ?","text":"<p>Le mod\u00e8le MeshSegNet est con\u00e7u pour prendre en entr\u00e9e des donn\u00e9es de maillage avec diff\u00e9rentes dimensions et effectuer une segmentation en plusieurs classes.  </p>"},{"location":"#3-augmentation-de-donnees-pour-des-fichiers-vtp","title":"3. Augmentation de donn\u00e9es pour des fichiers VTP","text":""},{"location":"#31-quest-ce-quil-presente","title":"3.1 Qu'est ce qu'il pr\u00e9sente ?","text":"<p>Une augmentation de donn\u00e9es pour des fichiers VTP (VTK PolyData) utilis\u00e9s dans un contexte de traitement de maillage 3D.  </p>"},{"location":"#32-quelle-utilite","title":"3.2 Quelle utilit\u00e9?","text":"<p>G\u00e9n\u00e9ration des versions augment\u00e9es de fichiers VTP 3D en appliquant des transformations al\u00e9atoires de rotation, translation  et mise \u00e0 l'\u00e9chelle \u00e0 chaque \u00e9chantillon d'origine. Ces augmentations peuvent \u00eatre utilis\u00e9es pour enrichir un ensemble de donn\u00e9es  d'entra\u00eenement pour l'apprentissage automatique, en particulier dans le contexte de la vision par ordinateur 3D.  </p> <pre><code>    def GetVTKTransformationMatrix(rotate_X=[-180, 180], rotate_Y=[-180, 180], \n                                   rotate_Z=[-180, 180],\n                                translate_X=[-10, 10], translate_Y=[-10, 10], \n                                translate_Z=[-10, 10],\n                                scale_X=[0.8, 1.2], scale_Y=[0.8, 1.2], \n                                scale_Z=[0.8, 1.]):\n</code></pre>"},{"location":"#4-splitage-des-donnees","title":"4. Splitage des donn\u00e9es","text":""},{"location":"#41-quest-ce-quil-presente","title":"4.1 Qu'est ce qu'il pr\u00e9sente ?","text":"<p>Un splitage des donn\u00e9es en donn\u00e9es d'entra\u00eenement et de validation.  </p>"},{"location":"#42-quelle-utilite","title":"4.2 Quelle utilit\u00e9?","text":"<p>Pour pr\u00e9parer nos donn\u00e9es en vue de l'entra\u00eenement du mod\u00e8le MeshSegNet, nous effectuons un processus de division des ensembles  de fichiers VTP augment\u00e9s en listes distinctes pour l'entra\u00eenement et la validation. Ces listes joueront un r\u00f4le crucial dans  le chargement des donn\u00e9es lors de la phase d'entra\u00eenement de notre mod\u00e8le d'apprentissage automatique.  </p>"},{"location":"#43-processus","title":"4.3 Processus ?","text":"<p>Pour expliquer davantage cette division, nous pouvons la d\u00e9composer en trois cat\u00e9gories principales : les donn\u00e9es d'entra\u00eenement, les donn\u00e9es de validation et les donn\u00e9es de test.  </p> <ul> <li>Les donn\u00e9es d'entra\u00eenement repr\u00e9sentent la portion du jeu de donn\u00e9es qui est sp\u00e9cifiquement utilis\u00e9e pour enseigner au mod\u00e8le. Ces donn\u00e9es servent de mat\u00e9riau d'apprentissage, permettant au mod\u00e8le d'acqu\u00e9rir des comp\u00e9tences et des connaissances.    </li> <li>Les donn\u00e9es de validation constituent un ensemble distinct de donn\u00e9es qui ne sont pas utilis\u00e9es pendant le processus d'entra\u00eenement  proprement dit. Elles sont r\u00e9serv\u00e9es exclusivement \u00e0 l'\u00e9valuation de la performance du mod\u00e8le tout au long de son apprentissage. Ces donn\u00e9es n'ont jamais \u00e9t\u00e9 vues par le mod\u00e8le auparavant, et leur utilisation permet de d\u00e9tecter toute tendance au surapprentissage.Le mod\u00e8le est r\u00e9guli\u00e8rement \u00e9valu\u00e9 sur ces donn\u00e9es de validation pendant l'entra\u00eenement.  </li> <li>Enfin, nous avons \u00e9galement isol\u00e9 un jeu de donn\u00e9es de test, compos\u00e9 de deux \u00e9chantillons distincts. Ces donn\u00e9es n'ont jamais \u00e9t\u00e9 expos\u00e9es au mod\u00e8le ni pendant la phase d'entra\u00eenement ni lors de l'\u00e9valuation sur les donn\u00e9es de validation. L'objectif principal de cet ensemble de test est de mesurer la performance finale du mod\u00e8le de mani\u00e8re objective.  </li> </ul>"},{"location":"#44-quoi-dautre","title":"4.4 Quoi d\u2019autre ?","text":"<p>Pour garantir la qualit\u00e9 du processus de division des donn\u00e9es, nous avons adopt\u00e9 une technique avanc\u00e9e appel\u00e9e cross-validation ou validation crois\u00e9e, qui implique un m\u00e9lange (shuffle) des donn\u00e9es avant la division. Cette approche vise \u00e0 \u00e9viter la cr\u00e9ation des sous-ensembles \u00e0 partir de donn\u00e9es tri\u00e9s ou structur\u00e9es d'une mani\u00e8re particuli\u00e8re. Cette d\u00e9marche est essentielle pour pr\u00e9venir tout biais potentiel dans l'\u00e9valuation du mod\u00e8le, en \u00e9vitant que les sous-ensembles  contiennent des donn\u00e9es similaires ou provenant de la m\u00eame classe.  </p> <pre><code>    train_size = 0.8  \n    train_list, val_list = train_test_split(sample_list, train_size=train_size, shuffle=True)\n</code></pre>"},{"location":"#5-processus-dentrainement-et-devaluation-du-modele","title":"5. Processus  d'entra\u00eenement et d'\u00e9valuation  du mod\u00e8le","text":""},{"location":"#51-quest-ce-quil-presente","title":"5.1 Qu'est ce qu'il pr\u00e9sente ?","text":"<p>Un processus  d'entra\u00eenement et d'\u00e9valuation d'un mod\u00e8le de segmentation pour des donn\u00e9es de maillage 3D.  </p>"},{"location":"#52-quels-sont-les-parametres-essentiels-pour-configurer-le-processus-dentrainement-du-modele","title":"5.2 Quels sont les param\u00e8tres essentiels pour configurer le processus d'entra\u00eenement du mod\u00e8le?","text":"<ul> <li>num_classes: Ce param\u00e8tre sp\u00e9cifie le nombre de classes ou de cat\u00e9gories diff\u00e9rentes que le mod\u00e8le de segmentation doit pr\u00e9dire. Dans ce cas, il y a 15 classes diff\u00e9rentes.   </li> <li>num_channels: Ce param\u00e8tre indique le nombre de canaux d'entr\u00e9e que le mod\u00e8le utilise pour chaque exemple de donn\u00e9es. Les canaux peuvent correspondre \u00e0 diff\u00e9rentes caract\u00e9ristiques ou informations dans les donn\u00e9es. Ici, il y a 15 canaux d'entr\u00e9e.   </li> <li>num_epochs : C'est le nombre total d'\u00e9poques d'entra\u00eenement que le mod\u00e8le va parcourir. Une \u00e9poque repr\u00e9sente une passe compl\u00e8te \u00e0 travers l'ensemble de donn\u00e9es d'entra\u00eenement.    </li> <li>train_batch_size: Le lot d'entra\u00eenement (ou mini-lot) est la quantit\u00e9 d'exemples de donn\u00e9es utilis\u00e9e \u00e0 chaque mise \u00e0 jour du mod\u00e8le pendant l'entra\u00eenement.   </li> <li>val_batch_size : C'est similaire au lot d'entra\u00eenement, mais cela s'applique aux donn\u00e9es de validation.    </li> <li>patch-size : Des r\u00e9gions d'int\u00e9r\u00eat dans les donn\u00e9es utilis\u00e9es pour l'entra\u00eenement du mod\u00e8le de segmentation.    </li> </ul> <pre><code>    num_classes = 15\n    num_channels = 15 \n    num_epochs = 20\n    train_batch_size = 2\n    val_batch_size = 2\n    num_batches_to_print = 20\n</code></pre> <pre><code>    training_dataset = Mesh_Dataset(data_list_path=train_list,\n                                    num_classes=num_classes,\n                                    patch_size=9000)\n    val_dataset = Mesh_Dataset(data_list_path=val_list,\n                               num_classes=num_classes,\n                               patch_size=9000)\n</code></pre> <p>Ces param\u00e8tres sont essentiels pour configurer le processus d'entra\u00eenement du mod\u00e8le, notamment le nombre de classes pr\u00e9dites, la taille des lots, le nombre d'\u00e9poques, le patch-size ... Ils peuvent \u00eatre ajust\u00e9s en fonction des besoins sp\u00e9cifiques de la t\u00e2che de segmentation et des ressources mat\u00e9rielles disponibles.  </p>"},{"location":"#6-la-segmentation-semantique-de-maillages-3d-sans-post-processing","title":"6. La segmentation s\u00e9mantique de maillages 3D sans post-processing","text":""},{"location":"#61-quest-ce-quil-presente","title":"6.1 Qu'est ce qu'il pr\u00e9sente ?","text":"<p>La segmentation s\u00e9mantique de maillages 3D en utilisant un mod\u00e8le de r\u00e9seau de neurones pr\u00e9-entra\u00een\u00e9 et sauvegarde les r\u00e9sultats  dans des fichiers de maillage de sortie.  </p>"},{"location":"#62-processus","title":"6.2 Processus?","text":"<ul> <li>pr\u00e9-traitement (pre-processing) : Ces \u00e9tapes de pr\u00e9-traitement sont importantes pour pr\u00e9parer les donn\u00e9es du maillage de mani\u00e8re appropri\u00e9e avant de les utiliser comme entr\u00e9e pour le mod\u00e8le de segmentation. Elles garantissent que les donn\u00e9es sont dans un format compatible avec le mod\u00e8le et peuvent am\u00e9liorer la qualit\u00e9 des pr\u00e9dictions du mod\u00e8le.   </li> <li>Downsampling (R\u00e9duction de l'\u00e9chantillonnage): Si le maillage d'entr\u00e9e a plus de 10 000 cellules, il est r\u00e9duit en \u00e9chantillonnant al\u00e9atoirement un sous-ensemble de 10 000 cellules. Cela permet de g\u00e9rer des maillages de grande taille de mani\u00e8re plus efficace.   </li> </ul> <pre><code>    if mesh.ncells &gt; 10000:\n        print('\\tDownsampling...')\n        target_num = 10000\n        ratio = target_num/mesh.ncells \n        mesh_d = mesh.clone()\n        mesh_d.decimate(fraction=ratio)\n        predicted_labels_d = np.zeros([mesh_d.ncells, 1], dtype=np.int32)\n    else:\n        mesh_d = mesh.clone()\n        predicted_labels_d = np.zeros([mesh_d.ncells, 1], dtype=np.int32)\n</code></pre> <ul> <li>Normalisation des coordonn\u00e9es : Les coordonn\u00e9es des points du maillage sont d\u00e9plac\u00e9es pour que le centre de masse du maillage soit \u00e0 l'origine (0, 0, 0). Cela permet de centrer le maillage et d'am\u00e9liorer la stabilit\u00e9 num\u00e9rique lors de la transformation en tenseurs pour le mod\u00e8le.   </li> </ul> <pre><code>    points = mesh_d.points()\n    mean_cell_centers = mesh_d.center_of_mass()\n    points[:, 0:3] -= mean_cell_centers[0:3]\n    ids = np.array(mesh_d.faces())\n    cells = points[ids].reshape(mesh_d.ncells, 9).astype(dtype='float32')\n</code></pre> <ul> <li>Normalisation des donn\u00e9es : Les coordonn\u00e9es des points et des cellules sont normalis\u00e9es en soustrayant la moyenne et en divisant par l'\u00e9cart type. Cela met \u00e0 l'\u00e9chelle les donn\u00e9es pour les rendre compatibles avec le mod\u00e8le et r\u00e9duit les probl\u00e8mes li\u00e9s \u00e0 l'\u00e9chelle.    </li> </ul> <pre><code>    maxs = points.max(axis=0)\n    mins = points.min(axis=0)\n    means = points.mean(axis=0)\n    stds = points.std(axis=0)\n    nmeans = normals.mean(axis=0)\n    nstds = normals.std(axis=0)\n    for i in range(3):\n        cells[:, i] = (cells[:, i] - means[i]) / stds[i] #point 1\n        cells[:, i+3] = (cells[:, i+3] - means[i]) / stds[i] #point 2\n        cells[:, i+6] = (cells[:, i+6] - means[i]) / stds[i] #point 3\n        barycenters[:,i] = (barycenters[:,i] - mins[i]) / (maxs[i]-mins[i])\n        normals[:,i] = (normals[:,i] - nmeans[i]) / nstds[i]\n    X = np.column_stack((cells, barycenters, normals))\n</code></pre> <ul> <li>Calcul des matrices d'adjacence (A_S et A_L) : Des matrices d'adjacence sont calcul\u00e9es en fonction des distances entre les points du maillage. Ces matrices sont utilis\u00e9es ult\u00e9rieurement dans le mod\u00e8le pour prendre en compte la connectivit\u00e9 entre les cellules.  </li> </ul> <pre><code>    A_S = np.zeros([X.shape[0], X.shape[0]], dtype='float32')\n    A_L = np.zeros([X.shape[0], X.shape[0]], dtype='float32')\n    D = distance_matrix(X[:, 9:12], X[:, 9:12])\n    A_S[D&lt;0.1] = 1.0\n    A_S = A_S / np.dot(np.sum(A_S, axis=1, keepdims=True), \n                           np.ones((1, X.shape[0])))\n    A_L[D&lt;0.2] = 1.0\n    A_L = A_L / np.dot(np.sum(A_L, axis=1, keepdims=True), \n                         np.ones((1, X.shape[0])))\n</code></pre> <ul> <li>Pr\u00e9dictions avec le mod\u00e8le      _ Les donn\u00e9es pr\u00e9par\u00e9es sont converties en tenseurs PyTorch et utilis\u00e9es en entr\u00e9e pour le mod\u00e8le MeshSegNet.  </li> </ul> <pre><code>    X = X.transpose(1, 0)\n    X = X.reshape([1, X.shape[0], X.shape[1]])\n    X = torch.from_numpy(X).to(device, dtype=torch.float)\n    A_S = A_S.reshape([1, A_S.shape[0], A_S.shape[1]])\n    A_L = A_L.reshape([1, A_L.shape[0], A_L.shape[1]])\n    A_S = torch.from_numpy(A_S).to(device, dtype=torch.float)\n    A_L = torch.from_numpy(A_L).to(device, dtype=torch.float)\n    tensor_prob_output = model(X, A_S, A_L).to(device, dtype=torch.float)\n</code></pre> <p>_ Le mod\u00e8le produit des probabilit\u00e9s de classe pour chaque cellule du maillage, et ces probabilit\u00e9s sont converties en \u00e9tiquettes de segmentation.   </p> <pre><code>    patch_prob_output = tensor_prob_output.cpu().numpy()\n    for i_label in range(num_classes):\n        predicted_labels_d[np.argmax(patch_prob_output[0, :], \n                               axis=-1)==i_label] = i_label\n</code></pre>"},{"location":"#7-la-segmentation-semantique-de-maillages-3d-avec-post-processing","title":"7. La segmentation s\u00e9mantique de maillages 3D avec post-processing","text":""},{"location":"#71-quest-ce-quil-presente","title":"7.1 Qu'est ce qu'il pr\u00e9sente ?","text":"<p>La segmentation s\u00e9mantique des maillages 3D en utilisant un mod\u00e8le pr\u00e9-entra\u00een\u00e9 (MeshSegNet) et effectue ensuite une s\u00e9rie d'\u00e9tapes de post-traitement pour affiner les pr\u00e9dictions et r\u00e9aliser un \u00e9chantillonnage des pr\u00e9dictions.  </p>"},{"location":"#72-processus","title":"7.2 Processus?","text":"<ul> <li>Downsampling (r\u00e9duction de l'\u00e9chantillonnage) : Le code commence par r\u00e9duire l'\u00e9chantillonnage en utilisant la m\u00e9thode de d\u00e9cimation pour r\u00e9duire le nombre de cellules dans le mod\u00e8le 3D. Cela permet de simplifier le mod\u00e8le tout en maintenant la structure globale.    </li> </ul> <pre><code>    print('\\tDownsampling...')\n    target_num = 10000\n    ratio = target_num/mesh.ncells\n    mesh_d = mesh.clone()\n    mesh_d.decimate(fraction=ratio)\n    predicted_labels_d = np.zeros([mesh_d.ncells, 1], dtype=np.int32)\n</code></pre> <ul> <li>Normalisation des donn\u00e9es : Les donn\u00e9es sont normalis\u00e9es en soustrayant la moyenne et en divisant par l'\u00e9cart type pour les points et les normales. Cela garantit que les donn\u00e9es ont une \u00e9chelle coh\u00e9rente.   </li> </ul> <pre><code>    maxs = points.max(axis=0)\n        mins = points.min(axis=0)\n        means = points.mean(axis=0)\n        stds = points.std(axis=0)\n        nmeans = normals.mean(axis=0)\n        nstds = normals.std(axis=0)\n        for i in range(3):\n            cells[:, i] = (cells[:, i] - means[i]) / stds[i] \n            cells[:, i+3] = (cells[:, i+3] - means[i]) / stds[i] \n            cells[:, i+6] = (cells[:, i+6] - means[i]) / stds[i] \n            barycenters[:,i] = (barycenters[:,i] - mins[i]) / (maxs[i]-mins[i])\n            normals[:,i] = (normals[:,i] - nmeans[i]) / nstds[i]\n</code></pre> <ul> <li>Pr\u00e9diction initiale : Le mod\u00e8le effectue une premi\u00e8re pr\u00e9diction des \u00e9tiquettes pour chaque cellule du mod\u00e8le r\u00e9duit.   </li> </ul> <pre><code>    for i_label in range(num_classes):\n        predicted_labels_d[np.argmax(patch_prob_output[0, :], \n                            axis=-1)==i_label] = i_label\n        mesh2 = mesh_d.clone()\n        mesh2.celldata['Label'] = predicted_labels_d\n        vedo.write(mesh2, os.path.join(output_path, \n        '{}_d_predicted.vtp'.format(i_sample[:-4])))\n</code></pre> <ul> <li>Refinement avec pygco : usage de l'algorithme d'optimisation par coupure de graphe (pygco - Graph Cuts Optimization) pour affiner les \u00e9tiquettes pr\u00e9dites. Cette technique d'optimisation consid\u00e8re \u00e0 la fois les valeurs unaires et les relations entre les paires d'\u00e9tiquettes afin d'am\u00e9liorer la pr\u00e9cision des \u00e9tiquettes. En g\u00e9n\u00e9ral, cet algorithme se base sur des co\u00fbts unaires, lesquels sont d\u00e9termin\u00e9s en fonction des probabilit\u00e9s de classe pr\u00e9dites par le mod\u00e8le. De plus, il prend en compte des co\u00fbts associ\u00e9s aux paires, lesquels sont calcul\u00e9s en fonction des relations spatiales entre les cellules. L'objectif est de trouver la configuration optimale des \u00e9tiquettes qui soit coh\u00e9rente avec les informations fournies par le mod\u00e8le et les relations spatiales existant entre les cellules.   </li> </ul> <pre><code>    print('\\tRefining by pygco...')\n    round_factor = 100\n    patch_prob_output[patch_prob_output&lt;1.0e-6] = 1.0e-6\n\n    unaries = -round_factor * np.log10(patch_prob_output)\n    unaries = unaries.astype(np.int32)\n    unaries = unaries.reshape(-1, num_classes)\n\n    pairwise = (1 - np.eye(num_classes, dtype=np.int32))\n\n    normals = mesh_d.celldata['Normals'].copy() \n    barycenters = mesh_d.cell_centers() \n    cell_ids = np.asarray(mesh_d.faces())\n\n    lambda_c = 30\n    edges = np.empty([1, 3], order='C')\n    for i_node in range(cells.shape[0]):\n        nei = np.sum(np.isin(cell_ids, cell_ids[i_node, :]), axis=1)\n        nei_id = np.where(nei==2)\n        for i_nei in nei_id[0][:]:\n            if i_node &lt; i_nei:\n                cos_theta = np.dot(normals[i_node, 0:3], normals[i_nei, 0:3])/\n                np.linalg.norm(normals[i_node, 0:3])/\n                                        np.linalg.norm(normals[i_nei, 0:3])\n                if cos_theta &gt;= 1.0:\n                    cos_theta = 0.9999\n                    theta = np.arccos(cos_theta)\n                    phi = np.linalg.norm\n                    (barycenters[i_node, :] - barycenters[i_nei, :])\n                    if theta &gt; np.pi/2.0:\n                        edges = np.concatenate((edges, np.array([i_node, i_nei, \n                        -np.log10(theta/np.pi)*phi]).reshape(1, 3)), axis=0)\n                    else:\n                        beta = 1 + np.linalg.norm(np.dot(normals[i_node, 0:3], \n                        normals[i_nei, 0:3]))\n                        edges = np.concatenate((edges, np.array([i_node, i_nei, \n                        -beta*np.log10(theta/np.pi)*phi]).reshape(1, 3)), \n                        axis=0)\n    edges = np.delete(edges, 0, 0)\n    edges[:, 2] *= lambda_c*round_factor\n    edges = edges.astype(np.int32)\n\n    refine_labels = cut_from_graph(edges, unaries, pairwise)\n    refine_labels = refine_labels.reshape([-1, 1])\n\n    mesh3 = mesh_d.clone()\n    mesh3.celldata['Label'] = refine_labels\n    vedo.write(mesh3, os.path.join(output_path, \n    '{}_d_predicted_refined.vtp'.format(i_sample[:-4])))\n</code></pre> <ul> <li>Upsampling :L'op\u00e9ration d'augmentation de l'\u00e9chantillonnage vise \u00e0 fournir des \u00e9tiquettes pour l'ensemble du mod\u00e8le 3D. Deux approches d'augmentation de l'\u00e9chantillonnage sont disponibles, \u00e0 savoir SVM et KNN. Cependant, avant de pouvoir effectuer cette augmentation de l'\u00e9chantillonnage, une pr\u00e9paration pr\u00e9liminaire des donn\u00e9es est n\u00e9cessaire pour chaque m\u00e9thode d'augmentation. Cette pr\u00e9paration inclut le calcul des caract\u00e9ristiques des cellules ou des \u00e9l\u00e9ments du mod\u00e8le r\u00e9duit, appel\u00e9s barycenters, qui serviront d'entr\u00e9e pour le processus d'augmentation.   </li> </ul> <pre><code>    print('\\tUpsampling...')\n    if mesh.ncells &gt; 50000:\n        target_num = 50000 \n        ratio = target_num/mesh.ncells \n        mesh.decimate(fraction=ratio)\n        print('Original contains too many cells, simpify to {} cells'\n                                .format(mesh.ncells))\n\n    barycenters = mesh3.cell_centers() \n    fine_barycenters = mesh.cell_centers() \n\n    if upsampling_method == 'SVM':\n        clf = SVC(kernel='rbf', gamma='auto')\n        clf.fit(barycenters, np.ravel(refine_labels))\n        fine_labels = clf.predict(fine_barycenters)\n        fine_labels = fine_labels.reshape(-1, 1)\n    elif upsampling_method == 'KNN':\n        neigh = KNeighborsClassifier(n_neighbors=3)\n        neigh.fit(barycenters, np.ravel(refine_labels))\n        fine_labels = neigh.predict(fine_barycenters)\n        fine_labels = fine_labels.reshape(-1, 1)\n\n        mesh.celldata['Label'] = fine_labels\n        vedo.write(mesh, os.path.join(output_path, \n        '{}_predicted_refined.vtp'.format(i_sample[:-4])))\n</code></pre> <p>Le post-traitement, \u00e9galement connu sous le nom de post-processing en anglais, est une \u00e9tape qui suit g\u00e9n\u00e9ralement une t\u00e2che de traitement ou d'analyse des donn\u00e9es. Elle vise \u00e0 am\u00e9liorer, corriger ou affiner les r\u00e9sultats obtenus apr\u00e8s l'ex\u00e9cution d'un algorithme ou d'une op\u00e9ration principale.</p>"},{"location":"#code-du-framework-meshsegnet-sur-google_drive","title":"Code du Framework MeshSegNet sur Google_drive","text":""},{"location":"#code-associe-google_colab","title":"Code associ\u00e9 Google_colab","text":""},{"location":"Deployment/","title":"Bienvenue dans notre documentation pour le d\u00e9ploiement","text":""},{"location":"Deployment/#fonctionnalites-du-projet","title":"Fonctionnalit\u00e9s du Projet","text":"<p>La partie de d\u00e9ploiement de notre projet est une \u00e9tape cruciale pour r\u00e9aliser des pr\u00e9dictions sur des objets 3D au format .vtp ou .obj et les visualiser dans notre site web. Cette phase est le pivot central de notre syst\u00e8me, permettant aux utilisateurs de tirer pleinement parti de la technologie 3D pour une exp\u00e9rience enrichissante.</p> <p>Notre objectif principal est d'offrir aux utilisateurs la capacit\u00e9 de t\u00e9l\u00e9charger des fichiers 3D, de les soumettre \u00e0 un processus de pr\u00e9diction, puis de visualiser les r\u00e9sultats de mani\u00e8re interactive sur notre site web. Pour atteindre cet objectif, nous avons mis en place une architecture en deux parties : le backend et le frontend.</p> <p>La partie backend : assure le traitement des donn\u00e9es, y compris la pr\u00e9diction \u00e0 l'aide du mod\u00e8le MeshSegNet.</p> <p>la partie frontend :offre une interface conviviale pour les utilisateurs</p>"},{"location":"Deployment/#configuration-et-installation","title":"Configuration et Installation","text":""},{"location":"Deployment/#1-configuration-de-lenvironnement-virtuel","title":"1. Configuration de l'environnement virtuel","text":"<p>Un environnement virtuel est un environnement Python isol\u00e9 qui permet de g\u00e9rer les d\u00e9pendances sp\u00e9cifiques \u00e0 un projet sans interf\u00e9rer avec d'autres projets Python sur le m\u00eame syst\u00e8me. Voici comment cr\u00e9er et configurer un environnement virtuel pour votre projet:</p>"},{"location":"Deployment/#-installation-de-loutil-virtualenv","title":"-Installation de l'outil  <code>virtualenv</code>:","text":"<p>Si vous n'avez pas d\u00e9j\u00e0 virtualenv install\u00e9 sur votre syst\u00e8me, vous pouvez l'installer en utilisant pip, qui est l'outil de gestion de paquets Python. Ouvrez un terminal et ex\u00e9cutez la commande suivante : <code>pip install virtualenv</code></p>"},{"location":"Deployment/#-creation-de-lenvironnement-virtuel","title":"-Cr\u00e9ation de l'environnement virtuel:","text":"<p>Dans le r\u00e9pertoire racine de votre projet, cr\u00e9ez un environnement virtuel en utilisant virtualenv. Remplacez nom_env par le nom que vous souhaitez donner \u00e0 votre environnement virtuel : virtualenv nom_env. Cela cr\u00e9era un dossier avec le nom de votre environnement virtuel contenant une installation Python propre et isol\u00e9e</p>"},{"location":"Deployment/#-activation-de-lenvironnement-virtuel","title":"-Activation de l'environnement virtuel:","text":"<p>Apr\u00e8s avoir cr\u00e9\u00e9 l'environnement virtuel, vous devez l'activer. Selon votre syst\u00e8me d'exploitation, la commande d'activation varie. Sur Windows: <code>nom_env\\Scripts\\activate.</code> Sur macOS ou Linux: <code>source nom_env/bin/activate</code></p>"},{"location":"Deployment/#2-installation-backend","title":"2. Installation Backend","text":""},{"location":"Deployment/#-installation-de-flask","title":"-Installation de Flask:","text":"<p>Dans l'environnement virtuel activ\u00e9 avant, utiliser lacommande suivante pour installer flask: <code>pip install flask</code></p>"},{"location":"Deployment/#-installation-des-dependances-python","title":"-Installation des d\u00e9pendances Python:","text":"<p>Utilisez pip pour installer les packages Python n\u00e9cessaires. Dans ce contexte, vous aurez probablement besoin de packages tels que Flask, NumPy, Torch, vedo, Pandas, Spicy... Exemple: <code>pip install flask numpy torch</code>.</p> <p>Pour lancer le serveur Backend, il faut acc\u00e9der au dossier de backend, et puis ex\u00e9cuter la commande <code>python app.py</code></p>"},{"location":"Deployment/#3-installation-frontend","title":"3. Installation Frontend","text":""},{"location":"Deployment/#-installation-de-nodejs-et-npm","title":"- Installation de Node.js et npm","text":"<p>Premi\u00e8rement, il faut t\u00e9l\u00e9charger le fichier Windows Installer(.msi) ou macOS Installer(.pkg), puis terminer le processus d' installation et la lancer.</p> <p>Pour lancer le serveur Frontend, il faut acc\u00e9der au dossier de fronted et ex\u00e9cuter la commande <code>npm start</code>.</p>"},{"location":"Deployment/#frontend","title":"Frontend","text":"<p>Cette partie est consacr\u00e9 pour entammer la partie Fontend de notre projet pour aider mieux \u00e0 sa comprehension</p>"},{"location":"Deployment/#1structure-du-frontend","title":"1.Structure du Frontend","text":"<p>l'arbrorescente du Frontend est la suivante:</p> <p></p>"},{"location":"Deployment/#vue-densemble","title":"Vue d'ensemble","text":"<p>-node modules:    Ce r\u00e9pertoire contient toutes les d\u00e9pendances externes install\u00e9es pour le projet \u00e0 l'aide de npm ou yarn.        <code>npm install</code></p> <p>-public:  Ce r\u00e9pertoire contient les fichiers statiques et publics du projet, tels que les fichiers HTML de base.</p> <p>-src:  C'est le r\u00e9pertoire principal du projet contenant le code source.</p> <p>assets: Ce r\u00e9pertoire contient des fichiers statiques utilis\u00e9s dans l'application, tels que les polices (font) et les images (img).</p> <p>components: Ce r\u00e9pertoire contient des composants React r\u00e9utilisables utilis\u00e9s pour construire l'interface utilisateur de l'application.</p> <p>About.js: Un composant qui affiche des informations sur l'application ou l'\u00e9quipe derri\u00e8re le projet.</p> <p>Banner.js: Un composant qui affiche  un texte  et une image anim\u00e9. </p> <p>Card.js: Un composant qui repr\u00e9sente une carte ou un \u00e9l\u00e9ment de contenu pouvant \u00eatre utilis\u00e9 pour afficher des informations.</p> <p>Contact.js: Un composant qui affiche un formulaire pour contacter l'\u00e9quipe de d\u00e9veloppement.</p> <p>Footer.js : Un composant qui repr\u00e9sente le pied de page de l'application, g\u00e9n\u00e9ralement avec des liens et des informations de copyright.</p> <p>NavBar.js: Un composant qui repr\u00e9sente la barre de navigation de l'application, g\u00e9n\u00e9ralement avec des liens vers les diff\u00e9rentes sections du site.</p> <p>Prediction.js: Un composant qui permet \u00e0 l'utilisateur de t\u00e9l\u00e9charger un fichier, de le t\u00e9l\u00e9verser vers un serveur, d'obtenir une pr\u00e9diction, de t\u00e9l\u00e9charger le r\u00e9sultat et d'afficher le r\u00e9sultat avec le composant Vis.</p> <p>Qui utilise le Hooks <code>UseState</code> pour g\u00e9rer plusieurs \u00e9tats, tels que selectedFile, predictionSuccess, predictionMessage, etc. Ces \u00e9tats sont utilis\u00e9s pour suivre et mettre \u00e0 jour l'\u00e9tat de la pr\u00e9diction.             Dans ce composant il existe 3 fonctions fondamentales :</p> <pre><code>           1- `HandleFileChange`: appel\u00e9 lorsque l'utilisateur s\u00e9lectionne le fichier a segment\u00e9 et elle met \u00e0 jour l'\u00e9tat `selectedFile` avec le fichier s\u00e9lectionn\u00e9.\n\n           2- `HandleUpload `:est appel\u00e9e lorsque l'utilisateur clique sur le bouton \"Predict\". Elle envoie le fichier t\u00e9l\u00e9charg\u00e9 vers un serveur distant \u00e0 l'aide d'une requ\u00eate HTTP POST en utilisant la biblioth\u00e8que axios\n\n\n            ``` \n              const response = await axios.post('http://127.0.0.1:5000/predict', formData, {\n             responseType: 'blob',\n              });\n          //  En cas de r\u00e9ussite, elle met \u00e0 jour les \u00e9tats predictionSuccess, predictionMessage, et predictionFileURL pour afficher le r\u00e9sultat de la pr\u00e9diction.\n             ```\n\n          3- `handleDownload`: appel\u00e9e lorsque l'utilisateur clique sur le bouton \"Download the VTP file\". Elle permet de t\u00e9l\u00e9charger le fichier r\u00e9sultant de la pr\u00e9diction.\n</code></pre> <p>et en fin la fonction  <code>scrollToVis</code> : Cette fonction fait d\u00e9filer la page vers le composant Vis qui affiche la visualisation r\u00e9sultante \u00e0 l'aide de VTk.js</p> <p>vis.js : Un composant charg\u00e9 pour la visualisation des donn\u00e9es r\u00e9sultantes de la pr\u00e9diction, en utilisant la biblioth\u00e8que vtk.js \u00e0 partir des donn\u00e9es re\u00e7ues.</p> <p>En r\u00e9sum\u00e9, ce composant  offre des fonctionnalit\u00e9s pour personnaliser la visualisation, ajuster les param\u00e8tres de repr\u00e9sentation,  \u00e0 l'aide de divers fonction telle que createViewer , createPipeline , updateRepresentation             ** , updateOpacity ..</p>"},{"location":"Deployment/#2interface-utilisateur","title":"2.Interface Utilisateur","text":""},{"location":"Deployment/#page-daccueil","title":"Page d'Accueil","text":""},{"location":"Deployment/#page-dinformations","title":"Page d'informations","text":""},{"location":"Deployment/#page-de-contact","title":"Page de contact","text":""},{"location":"Deployment/#page-de-segmentation","title":"Page de Segmentation.","text":""},{"location":"Deployment/#backend","title":"backend","text":""},{"location":"Deployment/#les-outils-necessaires","title":"lES OUTILS NECESSAIRES","text":""},{"location":"Deployment/#1-importations-des-bibliotheques","title":"1. Importations des biblioth\u00e8ques","text":"<p>Liste des biblioth\u00e8ques import\u00e9es dans le code, Il est important de noter que meshsegnet et losses_and_metrics_for_mesh sont des fichiers personnalis\u00e9s qui sont d\u00e9j\u00e0 utilis\u00e9s dans l'entra\u00eenement du mod\u00e8le .</p> <pre><code>import os\nfrom meshsegnet import *\nimport vedo\nfrom losses_and_metrics_for_mesh import *\nfrom scipy.spatial import distance_matrix\nfrom flask import Flask, request, send_from_directory, send_file\nfrom flask_cors import CORS\n</code></pre> <p><code>os</code>: Pour effectuer des op\u00e9rations li\u00e9es au syst\u00e8me de fichiers. meshsegnet: Importe les modules personnalis\u00e9s du fichier meshsegnet. Ce fichier contient  des d\u00e9finitions de mod\u00e8les ou d'autres fonctions sp\u00e9cifiques \u00e0 l'application.</p> <p><code>vedo</code>: Une biblioth\u00e8que pour la visualisation de maillages 3D. Il est utilis\u00e9 pour afficher des visualisations 3D.</p> <p><code>losses_and_metrics_for_mesh</code>: Importe les modules personnalis\u00e9s du fichier losses_and_metrics_for_mesh. Ce fichier contient  des fonctions pour calculer des m\u00e9triques ou des pertes sp\u00e9cifiques \u00e0 l'application.</p> <p><code>scipy.spatial.distance_matrix</code>: Une fonction pour calculer des matrices de distance spatiale.</p> <p><code>flask</code>: Un framework web Python pour cr\u00e9er des applications web. Il est utilis\u00e9 pour cr\u00e9er une application web Flask.</p> <p><code>flask_cors</code>: Une extension Flask pour g\u00e9rer la politique CORS (Cross-Origin Resource Sharing). Elle permet \u00e0 l'application d'accepter des requ\u00eates depuis diff\u00e9rents domaines.</p>"},{"location":"Deployment/#2-configuration-de-lapplication-flask","title":"2. Configuration de l'application Flask","text":"<p>L'application Flask est configur\u00e9e en utilisant les lignes de code suivantes :</p> <pre><code>app = Flask(__name__, static_folder='C:/Users/adnane/Desktop/Stage3D/frontend/build', static_url_path='/')\nCORS(app)\n</code></pre> <p><code>'__name__ '</code>: Il s'agit d'un param\u00e8tre sp\u00e9cial qui indique le nom du module ou du package actuel. Cela est g\u00e9n\u00e9ralement utilis\u00e9 pour aider Flask \u00e0 d\u00e9terminer les chemins vers les fichiers statiques et les templates. <code>static_folder</code>: Ce param\u00e8tre sp\u00e9cifie le dossier o\u00f9 les fichiers statiques de l'application sont stock\u00e9s. Dans ce cas, les fichiers statiques sont situ\u00e9s dans le r\u00e9pertoire 'C:/Users/adnane/Desktop/Stage3D/frontend/build'.</p> <p><code>static_url_path</code> : Ce param\u00e8tre d\u00e9finit le chemin d'acc\u00e8s URL sous lequel les fichiers statiques seront accessibles. Dans ce cas, les fichiers statiques sont accessibles sous le chemin URL '/'.</p> <p><code>CORS(app)</code> : Cette ligne de code active la prise en charge de CORS (Cross-Origin Resource Sharing) pour l'application Flask. CORS permet \u00e0 des ressources Web d'\u00eatre charg\u00e9es \u00e0 partir d'un domaine diff\u00e9rent de celui de l'application, ce qui est souvent n\u00e9cessaire lorsque l'application Flask communique avec des clients Web sur des domaines diff\u00e9rents. Cette activation de CORS autorise des requ\u00eates provenant de domaines externes \u00e0 acc\u00e9der aux ressources de l'application Flask.</p>"},{"location":"Deployment/#3-variables-globales","title":"3. Variables globales","text":"<p>Ce bloc de code d\u00e9finit un ensemble de variables globales utilis\u00e9es dans l'application Flask. </p> <pre><code>upsampling_method = 'KNN'\nmodel_path = './models'\nmodel_name = 'MeshSegNet_Max_15_classes_72samples_lr1e-2_best.zip'\nmesh_path = './data_test'  # need to define\nsample_filename = \"\"\noutput_path = './outputs'\nif not os.path.exists(output_path):\nos.mkdir(output_path)\nnum_classes = 15\nnum_channels = 15\ndevice = torch.device('cpu')\n\n</code></pre> <p><code>upsampling_method</code>: Cette variable est initialis\u00e9e avec la cha\u00eene de caract\u00e8res 'KNN', ce qui sugg\u00e8re qu'elle est utilis\u00e9e pour sp\u00e9cifier une m\u00e9thode d'upsampling. Cela peut \u00eatre une indication que l'application effectue une certaine forme de traitement de donn\u00e9es ou d'interpolation, peut-\u00eatre dans le contexte de donn\u00e9es 3D.</p> <p><code>model_path</code>: Cette variable contient un chemin relatif ('./models') qui indique le r\u00e9pertoire o\u00f9 sont stock\u00e9s les mod\u00e8les. Il semble que l'application utilise des mod\u00e8les pr\u00e9-entra\u00een\u00e9s pour effectuer des pr\u00e9dictions ou des classifications.</p> <p><code>model_name</code>: Cette variable contient le nom du mod\u00e8le sp\u00e9cifique ('MeshSegNet_Max_15_classes_72samples_lr1e-2_best.zip'). Il s'agit probablement du nom du mod\u00e8le pr\u00e9-entra\u00een\u00e9 que l'application chargera pour effectuer des op\u00e9rations de pr\u00e9diction ou de classification.</p> <p><code>mesh_path</code>: Cette variable contient un chemin relatif ('./data_test') qui indique le r\u00e9pertoire o\u00f9 se trouvent des donn\u00e9es mesh ou des fichiers 3D. Cependant, la variable est laiss\u00e9e non d\u00e9finie ('need to define'), ce qui signifie que le chemin r\u00e9el doit \u00eatre sp\u00e9cifi\u00e9 ult\u00e9rieurement.</p> <p><code>sample_filename</code>: Cette variable est initialis\u00e9e avec une cha\u00eene de caract\u00e8res vide ('\"\"'). Il semble que cette variable sera utilis\u00e9e pour stocker le nom de fichier d'un \u00e9chantillon ou d'une donn\u00e9e mesh sp\u00e9cifique.</p> <p><code>output_path</code>: Cette variable contient un chemin relatif ('./outputs') qui indique le r\u00e9pertoire o\u00f9 les r\u00e9sultats de l'application seront stock\u00e9s. Si le r\u00e9pertoire n'existe pas, le code cr\u00e9e ce r\u00e9pertoire \u00e0 l'aide de os.mkdir(output_path).</p> <p><code>num_classes</code> : Cette variable est initialis\u00e9e \u00e0 15. Elle semble indiquer le nombre de classes ou de cat\u00e9gories diff\u00e9rentes que le mod\u00e8le ou l'application traitera. Dans le contexte de l'apprentissage automatique, cela peut signifier qu'il y a 15 classes diff\u00e9rentes \u00e0 pr\u00e9dire ou \u00e0 classer.</p> <p><code>num_channels</code>: Cette variable est \u00e9galement initialis\u00e9e \u00e0 15. Le terme \"channels\" (canaux) est g\u00e9n\u00e9ralement utilis\u00e9 dans le contexte de donn\u00e9es multidimensionnelles, comme les images ou les donn\u00e9es 3D. Cela peut signifier qu'il y a 15 canaux ou 15 caract\u00e9ristiques diff\u00e9rentes dans les donn\u00e9es d'entr\u00e9e ou dans le mod\u00e8le.</p> <p><code>device</code>: Cette variable est initialis\u00e9e avec l'objet torch.device('cpu'), ce qui signifie que l'application ou le mod\u00e8le sera ex\u00e9cut\u00e9 sur le processeur central (CPU) de l'ordinateur plut\u00f4t que sur un processeur graphique (GPU). L'utilisation du CPU est courante lorsque les ressources GPU ne sont pas disponibles ou n\u00e9cessaires pour des t\u00e2ches sp\u00e9cifiques.</p>"},{"location":"Deployment/#process-de-lapplication","title":"PROCESS DE L'APPLICATION","text":""},{"location":"Deployment/#1-chargement-du-modele-pre-entraine","title":"1. Chargement du mod\u00e8le pr\u00e9-entra\u00een\u00e9","text":"<p>Ces lignes de code concernent la cr\u00e9ation et le chargement d'un mod\u00e8le d'apprentissage automatique.</p> <pre><code>model = MeshSegNet(num_classes=num_classes, num_channels=num_channels).to(device, dtype=torch.float)\ncheckpoint = torch.load(os.path.join(model_path, model_name), map_location='cpu')\nmodel.load_state_dict(checkpoint['model_state_dict'])\ndel checkpoint\nmodel = model.to(device, dtype=torch.float)\n</code></pre> <p><code>model = MeshSegNet(num_classes=num_classes, num_channels=num_channels).to(device, dtype=torch.float)</code> : Cette ligne cr\u00e9e une instance du mod\u00e8le MeshSegNet avec les param\u00e8tres num_classes et num_channels pr\u00e9c\u00e9demment d\u00e9finis. Le mod\u00e8le est ensuite d\u00e9plac\u00e9 sur le dispositif sp\u00e9cifi\u00e9 (CPU dans ce cas) et son type de donn\u00e9es est d\u00e9fini sur torch.float.</p> <p><code>checkpoint = torch.load(os.path.join(model_path, model_name), map_location='cpu') :</code> Cette ligne charge un checkpoint (une sauvegarde) du mod\u00e8le depuis un fichier sp\u00e9cifi\u00e9 par model_path et model_name. Le param\u00e8tre map_location='cpu' indique que le mod\u00e8le doit \u00eatre charg\u00e9 sur le CPU, m\u00eame s'il a \u00e9t\u00e9 enregistr\u00e9 sur un GPU.</p> <p><code>model.load_state_dict(checkpoint['model_state_dict'])</code>: Cette ligne charge les poids et les param\u00e8tres du mod\u00e8le \u00e0 partir du dictionnaire model_state_dict contenu dans le checkpoint. Cela initialise effectivement le mod\u00e8le avec les poids pr\u00e9-entra\u00een\u00e9s.</p> <p><code>del checkpoint</code>: Cette ligne supprime l'objet checkpoint de la m\u00e9moire une fois que les poids du mod\u00e8le ont \u00e9t\u00e9 charg\u00e9s. Cela lib\u00e8re de la m\u00e9moire car le checkpoint n'est plus n\u00e9cessaire.</p> <p><code>model = model.to(device, dtype=torch.float) :</code>Cette ligne d\u00e9place \u00e0 nouveau le mod\u00e8le sur le CPU et d\u00e9finit son type de donn\u00e9es sur torch.float. Bien que cette ligne semble redondante par rapport \u00e0 la premi\u00e8re ligne, elle est importante car le chargement du checkpoint peut potentiellement modifier le dispositif sur lequel le mod\u00e8le est situ\u00e9, il est donc pr\u00e9f\u00e9rable de s'assurer qu'il est sur le CPU \u00e0 la fin du chargement.</p>"},{"location":"Deployment/#2-fonction-de-traitement-de-la-prediction","title":"2. Fonction de traitement de la pr\u00e9diction","text":"<p>La fonction process_prediction(uploaded_file) \u00eatre responsable du traitement des fichiers de pr\u00e9diction envoy\u00e9s via une requ\u00eate POST.</p> <pre><code>        uploaded_file = request.files['meshFile']\n        if uploaded_file.filename.lower().endswith('.vtp'):\n            i_sample = '1.vtp'\n        elif uploaded_file.filename.lower().endswith('.obj'):\n            i_sample = '1.obj'\n        print('Predicting Sample filename: {}'.format(i_sample))\n        mesh = vedo.load(os.path.join(mesh_path, i_sample))\n</code></pre> <p><code>uploaded_file = request.files['meshFile'] :</code> Cette ligne r\u00e9cup\u00e8re le fichier t\u00e9l\u00e9charg\u00e9 \u00e0 partir de la requ\u00eate HTTP en utilisant la cl\u00e9 'meshFile'. La variable uploaded_file contient maintenant les informations sur le fichier t\u00e9l\u00e9charg\u00e9.</p> <p><code>if uploaded_file.filename.lower().endswith('.vtp'):</code>: Cette condition v\u00e9rifie si le nom du fichier t\u00e9l\u00e9charg\u00e9 se termine par l'extension .vtp. Elle utilise uploaded_file.filename pour obtenir le nom du fichier, le convertit en minuscules avec .lower(), puis utilise .endswith('.vtp') pour v\u00e9rifier l'extension. Si c'est le cas, la variable i_sample est d\u00e9finie sur '1.vtp'.</p> <p><code>elif uploaded_file.filename.lower().endswith('.obj'):</code> : Sinon, cette condition v\u00e9rifie si le nom du fichier t\u00e9l\u00e9charg\u00e9 se termine par l'extension .obj. Si c'est le cas, la variable i_sample est d\u00e9finie sur '1.obj'.</p> <p><code>print('Predicting Sample filename: {}'.format(i_sample)) :</code> Cette ligne affiche le nom du fichier i_sample, qui sera utilis\u00e9 pour la pr\u00e9diction. Cela permet de suivre quel fichier est en cours de traitement.</p> <p><code>mesh = vedo.load(os.path.join(mesh_path, i_sample)) :</code>Cette ligne charge le fichier mesh \u00e0 partir du chemin complet du fichier i_sample, en utilisant la biblioth\u00e8que vedo. Le mesh charg\u00e9 est stock\u00e9 dans la variable mesh pour \u00eatre utilis\u00e9 dans la suite du code.</p> <pre><code>        if mesh.ncells &gt; 10000:\n            print('\\tDownsampling...')\n            target_num = 10000\n            ratio = target_num / mesh.ncells  # calculate ratio\n            mesh_d = mesh.clone()\n            mesh_d.decimate(fraction=ratio)\n            predicted_labels_d = np.zeros([mesh_d.ncells, 1], dtype=np.int32)\n        else:\n            mesh_d = mesh.clone()\n            predicted_labels_d = np.zeros([mesh_d.ncells, 1], dtype=np.int32)\n\n</code></pre> <p><code>if mesh.ncells &gt; 10000:</code> : Cette condition v\u00e9rifie si le nombre de cellules dans le mesh (mesh.ncells) est sup\u00e9rieur \u00e0 10 000. Si c'est le cas, cela signifie que le mesh est trop dense, et un \u00e9chantillonnage est n\u00e9cessaire pour r\u00e9duire sa complexit\u00e9.</p> <p><code>print('\\tDownsampling...')</code> Si l'\u00e9chantillonnage est n\u00e9cessaire, cette ligne affiche un message \"Downsampling...\" pour indiquer qu'un \u00e9chantillonnage est en cours.</p> <p><code>target_num = 10000</code> Cette ligne d\u00e9finit le nombre cible de cellules apr\u00e8s l'\u00e9chantillonnage. Dans ce cas, le nombre cible est de 10 000 cellules.</p> <p><code>ratio = target_num / mesh.ncells</code> Cette ligne calcule le ratio entre le nombre cible de cellules (target_num) et le nombre de cellules actuel du mesh (mesh.ncells). Ce ratio sera utilis\u00e9 pour d\u00e9terminer combien de cellules doivent \u00eatre conserv\u00e9es lors de l'\u00e9chantillonnage.</p> <p><code>mesh_d = mesh.clone() :</code> Cette ligne cr\u00e9e une copie du mesh d'origine en utilisant la m\u00e9thode .clone(). La copie sera utilis\u00e9e pour effectuer l'\u00e9chantillonnage tout en pr\u00e9servant le mesh d'origine.</p> <p><code>mesh_d.decimate(fraction=ratio)</code> : Cette ligne effectue l'\u00e9chantillonnage (downsampling) du mesh en r\u00e9duisant le nombre de cellules en fonction du ratio calcul\u00e9. La m\u00e9thode .decimate() r\u00e9duit la complexit\u00e9 du mesh en supprimant certaines cellules tout en pr\u00e9servant la structure globale du mesh.</p> <p><code>predicted_labels_d = np.zeros([mesh_d.ncells, 1], dtype=np.int32) :</code> Cette ligne cr\u00e9e un tableau numpy rempli de z\u00e9ros de taille [mesh_d.ncells, 1]. Ce tableau sera utilis\u00e9 pour stocker les \u00e9tiquettes pr\u00e9dites apr\u00e8s la pr\u00e9diction du mod\u00e8le. La taille du tableau est bas\u00e9e sur le nombre de cellules du mesh \u00e9chantillonn\u00e9 (mesh_d.ncells). <code>mesh_d = mesh.clone()</code> : Dans ce cas, une copie du mesh d'origine est \u00e9galement cr\u00e9\u00e9e, mais aucun \u00e9chantillonnage n'est effectu\u00e9. Le mesh \u00e9chantillonn\u00e9 (mesh_d) est identique au mesh d'origine, et le tableau predicted_labels_d est \u00e9galement initialis\u00e9 \u00e0 z\u00e9ro.</p> <pre><code>        print('\\tPredicting...')\n        points = mesh_d.points()\n        mean_cell_centers = mesh_d.center_of_mass()\n        points[:, 0:3] -= mean_cell_centers[0:3]\n        ids = np.array(mesh_d.faces())\n        cells = points[ids].reshape(mesh_d.ncells, 9).astype(dtype='float32')\n        mesh_d.compute_normals()\n        normals = mesh_d.celldata['Normals']\n        barycenters = mesh_d.cell_centers()  # don't need to copy\n        barycenters -= mean_cell_centers[0:3]\n</code></pre> <p><code>print('\\tPredicting...')</code> Cette ligne affiche un message \"Predicting...\" pour indiquer que le processus de pr\u00e9diction commence.</p> <p><code>points = mesh_d.points()</code>  Cette ligne extrait les coordonn\u00e9es des points du mesh \u00e9chantillonn\u00e9 et les stocke dans la variable points.</p> <p>mean_cell_centers = mesh_d.center_of_mass() ` Cette ligne calcule le centre de masse (barycentre) du mesh \u00e9chantillonn\u00e9 mesh_d en utilisant la m\u00e9thode center_of_mass(). Le r\u00e9sultat est stock\u00e9 dans mean_cell_centers.</p> <p><code>points[:, 0:3] -= mean_cell_centers[0:3]</code> Cette ligne centre le mesh autour de son centre de masse en soustrayant les coordonn\u00e9es du centre de masse mean_cell_centers des coordonn\u00e9es de chaque point dans points. Cela permet de ramener le centre du mesh \u00e0 l'origine (0, 0, 0).</p> <p><code>ids = np.array(mesh_d.faces())</code> Cette ligne extrait les indices des faces du mesh \u00e9chantillonn\u00e9 et les stocke dans ids.</p> <p><code>cells = points[ids].reshape(mesh_d.ncells, 9).astype(dtype='float32')</code> Cette ligne utilise les indices des faces (ids) pour extraire les coordonn\u00e9es des sommets des triangles qui composent le mesh \u00e9chantillonn\u00e9. Les coordonn\u00e9es sont ensuite mises en forme dans un tableau de forme [mesh_d.ncells, 9] o\u00f9 chaque ligne repr\u00e9sente les coordonn\u00e9es des trois sommets d'un triangle. Le tableau est converti en type de donn\u00e9es float32.</p> <p><code>mesh_d.compute_normals()</code> : Cette ligne calcule les normales (vecteurs perpendiculaires) des faces du mesh \u00e9chantillonn\u00e9 mesh_d en utilisant la m\u00e9thode compute_normals(). Les normales sont utilis\u00e9es pour d\u00e9terminer l'orientation des surfaces.</p> <p><code>normals = mesh_d.celldata['Normals']</code> Cette ligne extrait les normales calcul\u00e9es \u00e0 partir du mesh \u00e9chantillonn\u00e9 mesh_d et les stocke dans la variable normals.</p> <p><code>barycenters = mesh_d.cell_centers()</code> Cette ligne calcule les centres de chaque cellule (triangle) dans le mesh \u00e9chantillonn\u00e9 mesh_d en utilisant la m\u00e9thode cell_centers(). Ces centres de cellules sont stock\u00e9s dans barycenters.</p> <p><code>barycenters -= mean_cell_centers[0:3]</code> Cette ligne centre \u00e9galement les centres de cellules autour du centre de masse du mesh en soustrayant les coordonn\u00e9es du centre de masse mean_cell_centers des coordonn\u00e9es des centres de cellules. Cela garantit que le mesh et les centres de cellules sont centr\u00e9s autour de l'origine.</p> <pre><code>        A_S = np.zeros([X.shape[0], X.shape[0]], dtype='float32')\n        A_L = np.zeros([X.shape[0], X.shape[0]], dtype='float32')\n        D = distance_matrix(X[:, 9:12], X[:, 9:12])\n        A_S[D &lt; 0.1] = 1.0\n        A_S = A_S / np.dot(np.sum(A_S, axis=1, keepdims=True), np.ones((1, X.shape[0])))\n        A_L[D &lt; 0.2] = 1.0\n        A_L = A_L / np.dot(np.sum(A_L, axis=1, keepdims=True), np.ones((1, X.shape[0])))\n</code></pre> <p><code>A_S = np.zeros([X.shape[0], X.shape[0]], dtype='float32') :</code> Cela cr\u00e9e une matrice carr\u00e9e A_S remplie de z\u00e9ros, avec une dimension \u00e9gale au nombre de points dans le mesh. Cette matrice repr\u00e9sente l'adjacence \u00e0 courte port\u00e9e.</p> <p><code>A_L = np.zeros([X.shape[0], X.shape[0]], dtype='float32') :</code>De m\u00eame, cela cr\u00e9e une matrice carr\u00e9e A_L remplie de z\u00e9ros, avec la m\u00eame dimension que A_S. Cette matrice repr\u00e9sente l'adjacence \u00e0 longue port\u00e9e.</p> <p><code>D = distance_matrix(X[:, 9:12], X[:, 9:12]) :</code> Cela calcule une matrice de distances D en utilisant la fonction distance_matrix de SciPy. Les colonnes de X sont extraites pour obtenir les coordonn\u00e9es 3D des points (9:12 correspond aux colonnes des coordonn\u00e9es dans X).</p> <p><code>A_S[D &lt; 0.1] = 1.0 :</code> Cette ligne met \u00e0 1.0 toutes les entr\u00e9es de la matrice A_S o\u00f9 les distances dans D sont inf\u00e9rieures \u00e0 0,1 unit\u00e9. Cela signifie que les points situ\u00e9s \u00e0 moins de 0,1 unit\u00e9 les uns des autres sont consid\u00e9r\u00e9s comme voisins \u00e0 courte port\u00e9e.</p> <p><code>A_S = A_S / np.dot(np.sum(A_S, axis=1, keepdims=True), np.ones((1, X.shape[0]))) :</code> Cette ligne normalise la matrice A_S en divisant chaque ligne par la somme de ses \u00e9l\u00e9ments. Cela garantit que chaque ligne de la matrice A_S repr\u00e9sente une distribution de probabilit\u00e9 sur les voisins \u00e0 courte port\u00e9e pour chaque point.</p> <p><code>A_L[D &lt; 0.2] = 1.0 :</code> De mani\u00e8re similaire \u00e0 A_S, cette ligne met \u00e0 1.0 toutes les entr\u00e9es de la matrice A_L o\u00f9 les distances dans D sont inf\u00e9rieures \u00e0 0,2 unit\u00e9. Cela signifie que les points situ\u00e9s \u00e0 moins de 0,2 unit\u00e9 les uns des autres sont consid\u00e9r\u00e9s comme voisins \u00e0 longue port\u00e9e.</p> <p><code>A_L = A_L / np.dot(np.sum(A_L, axis=1, keepdims=True), np.ones((1, X.shape[0])))</code> : Comme pour A_S, cette ligne normalise la matrice A_L pour obtenir une distribution de probabilit\u00e9 sur les voisins \u00e0 longue port\u00e9e pour chaque point.</p> <pre><code>        X = X.transpose(1, 0)\n        X = X.reshape([1, X.shape[0], X.shape[1]])\n        X = torch.from_numpy(X).to(device, dtype=torch.float)\n        A_S = A_S.reshape([1, A_S.shape[0], A_S.shape[1]])\n        A_L = A_L.reshape([1, A_L.shape[0], A_L.shape[1]])\n        A_S = torch.from_numpy(A_S).to(device, dtype=torch.float)\n        A_L = torch.from_numpy(A_L).to(device, dtype=torch.float)\n        tensor_prob_output = model(X, A_S, A_L).to(device, dtype=torch.float)\n        patch_prob_output = tensor_prob_output.cpu().numpy()\n        for i_label in range(num_classes):\n            predicted_labels_d[np.argmax(patch_prob_output[0, :], axis=-1) == i_label] = i_label\n</code></pre> <p><code>X = X.transpose(1, 0)</code> : Cette ligne transpose la matrice X de sorte que les dimensions 0 et 1 soient \u00e9chang\u00e9es. Ensuite, la matrice X est r\u00e9organis\u00e9e pour avoir une dimension suppl\u00e9mentaire \u00e0 l'avant (ajoutant un axe) en utilisant reshape(). Ces \u00e9tapes sont n\u00e9cessaires pour pr\u00e9parer les donn\u00e9es d'entr\u00e9e du mod\u00e8le de mani\u00e8re appropri\u00e9e.</p> <p><code>X = torch.from_numpy(X).to(device, dtype=torch.float)</code> : Cette ligne convertit la matrice NumPy X en un tenseur PyTorch et le d\u00e9place vers le p\u00e9riph\u00e9rique sp\u00e9cifi\u00e9 par device (CPU ou GPU) en utilisant .to(). De plus, le type de donn\u00e9es du tenseur est d\u00e9fini sur torch.float.</p> <p><code>A_S = A_S.reshape([1, A_S.shape[0], A_S.shape[1]])</code>: Cette ligne r\u00e9organise la matrice NumPy A_S pour avoir une dimension suppl\u00e9mentaire \u00e0 l'avant (ajoutant un axe). Cela est n\u00e9cessaire pour pr\u00e9parer les donn\u00e9es d'adjacence \u00e0 \u00eatre compatibles avec le mod\u00e8le.</p> <p><code>A_L = A_L.reshape([1, A_L.shape[0], A_L.shape[1]])</code>: De m\u00eame, cette ligne r\u00e9organise la matrice NumPy A_L pour avoir une dimension suppl\u00e9mentaire \u00e0 l'avant.</p> <p><code>A_S = torch.from_numpy(A_S).to(device, dtype=torch.float)</code>: Comme pr\u00e9c\u00e9demment, cette ligne convertit la matrice NumPy A_S en un tenseur PyTorch et le d\u00e9place vers le p\u00e9riph\u00e9rique sp\u00e9cifi\u00e9 par device en d\u00e9finissant le type de donn\u00e9es sur torch.float.</p> <p><code>A_L = torch.from_numpy(A_L).to(device, dtype=torch.float)</code> : Cette ligne fait la m\u00eame chose pour la matrice NumPy A_L.</p> <p><code>tensor_prob_output = model(X, A_S, A_L).to(device, dtype=torch.float)</code>: Cette ligne effectue l'inf\u00e9rence du mod\u00e8le en passant les donn\u00e9es d'entr\u00e9e X, A_S, et A_L au mod\u00e8le model. Le r\u00e9sultat est un tenseur PyTorch contenant les probabilit\u00e9s pr\u00e9dites pour chaque classe. Le tenseur r\u00e9sultant est \u00e9galement d\u00e9plac\u00e9 vers le p\u00e9riph\u00e9rique sp\u00e9cifi\u00e9 et son type de donn\u00e9es est fix\u00e9 \u00e0 torch.float.</p> <p><code>patch_prob_output = tensor_prob_output.cpu().numpy()</code>: Cette ligne convertit le tenseur PyTorch tensor_prob_output en un tableau NumPy en utilisant .cpu().numpy(). Cela est n\u00e9cessaire car les op\u00e9rations ult\u00e9rieures seront effectu\u00e9es en utilisant NumPy.</p> <p><code>for i_label in range(num_classes) :</code> Cette boucle it\u00e8re sur toutes les classes possibles (num_classes) pour effectuer des op\u00e9rations sp\u00e9cifiques pour chaque classe.</p> <pre><code>        mesh2 = mesh_d.clone()\n        mesh2.celldata['Label'] = predicted_labels_d\n        output_file_path = os.path.join(output_path, '1.vtp')\n        vedo.write(mesh2, output_file_path)\n        print('Sample filename: {} completed'.format(i_sample))\n        return output_file_path\n</code></pre> <p><code>mesh2 = mesh_d.clone()</code> Cette ligne cr\u00e9e une copie du mesh \u00e9chantillonn\u00e9 mesh_d en utilisant la m\u00e9thode clone(). La nouvelle copie est stock\u00e9e dans mesh2.</p> <p><code>mesh2.celldata['Label'] = predicted_labels_d</code>Cette ligne ajoute des donn\u00e9es de cellule appel\u00e9es 'Label' \u00e0 mesh2. Les valeurs de 'Label' sont d\u00e9finies \u00e0 partir du tableau predicted_labels_d, qui contient les \u00e9tiquettes pr\u00e9dites pour chaque cellule du mesh. Ainsi, chaque cellule du mesh est \u00e9tiquet\u00e9e avec la classe pr\u00e9dite.</p> <p><code>output_file_path = os.path.join(output_path, '1.vtp')</code> Cette ligne d\u00e9finit le chemin du fichier de sortie au format VTP (VTK Polygonal Data). Le fichier sera enregistr\u00e9 dans le r\u00e9pertoire de sortie sp\u00e9cifi\u00e9 par output_path avec le nom '1.vtp'.</p> <p><code>vedo.write(mesh2, output_file_path)</code> Cette ligne \u00e9crit le mesh mesh2, avec les donn\u00e9es de cellule 'Label' ajout\u00e9es, dans le fichier de sortie sp\u00e9cifi\u00e9 par output_file_path. La fonction vedo.write() est utilis\u00e9e pour effectuer cette op\u00e9ration.</p> <p><code>print('Sample filename: {} completed'.format(i_sample))</code> Cette ligne affiche un message indiquant que la pr\u00e9diction pour l'\u00e9chantillon sp\u00e9cifique (d\u00e9termin\u00e9 par i_sample) est termin\u00e9e.</p> <p><code>return output_file_path</code> Cette ligne retourne le chemin du fichier de sortie au format VTP g\u00e9n\u00e9r\u00e9. Ce chemin sera utilis\u00e9 pour envoyer le fichier VTP en r\u00e9ponse \u00e0 la requ\u00eate POST.</p>"},{"location":"Deployment/#3-telechargement-de-fichiers","title":"3. T\u00e9l\u00e9chargement de fichiers","text":"<p>La fonction download(uploaded_file)  \u00eatre utilis\u00e9e pour sauvegarder un fichier t\u00e9l\u00e9charg\u00e9 sur le syst\u00e8me de fichiers.</p> <pre><code>    def download(uploaded_file):\n       if uploaded_file.filename != '':\n           if uploaded_file.filename.lower().endswith('.vtp'):\n            filename = '1.vtp'\n           elif uploaded_file.filename.lower().endswith('.obj'):\n            filename = '1.obj'\n\n        uploaded_file.save(os.path.join(mesh_path, filename))\n        global sample_filenames\n        sample_filenames = [filename]\n</code></pre> <p><code>La fonction download(uploaded_file)</code>  \u00eatre utilis\u00e9e pour sauvegarder un fichier t\u00e9l\u00e9charg\u00e9 sur le syst\u00e8me de fichiers. if uploaded_file.filename != '': : Cette condition v\u00e9rifie si le nom du fichier t\u00e9l\u00e9charg\u00e9 (uploaded_file.filename) n'est pas une cha\u00eene vide, ce qui signifie qu'un fichier a \u00e9t\u00e9 t\u00e9l\u00e9charg\u00e9.</p> <p><code>if uploaded_file.filename.lower().endswith('.vtp'):</code> : Cette condition v\u00e9rifie si le nom du fichier t\u00e9l\u00e9charg\u00e9 a une extension .vtp. Si c'est le cas, la variable filename est d\u00e9finie sur '1.vtp'. Sinon, si l'extension est .obj, la variable filename est d\u00e9finie sur '1.obj'.</p> <p><code>uploaded_file.save(os.path.join(mesh_path, filename)) :</code> Cette ligne enregistre le fichier t\u00e9l\u00e9charg\u00e9 dans le r\u00e9pertoire mesh_path avec le nom de fichier filename. Cela permet de stocker le fichier sur le syst\u00e8me de fichiers.</p> <p><code>global sample_filenames :</code> Cette ligne indique que vous allez utiliser une variable globale sample_filenames pour stocker le nom du fichier. Cette variable semble \u00eatre utilis\u00e9e ailleurs dans le code.</p> <p><code>sample_filenames = [filename] :</code> Cette ligne cr\u00e9e une liste sample_filenames contenant le nom du fichier filename. Cela peut \u00eatre utile pour garder une trace des fichiers t\u00e9l\u00e9charg\u00e9s pour le traitement ult\u00e9rieur.</p>"},{"location":"Deployment/#le-benchmarking-des-technologies-utilisees","title":"Le Benchmarking des Technologies Utilis\u00e9es","text":""},{"location":"Deployment/#1-flask-vs-django-pour-le-backend","title":"1. Flask vs. Django (pour le backend)","text":"<ul> <li> <p>Flask : Flask est un framework Python l\u00e9ger et minimaliste pour la cr\u00e9ation d'API web. Il convient aux applications de petite \u00e0 moyenne taille.  </p> </li> <li> <p>Django : Django est un framework web Python plus complet qui offre plus de fonctionnalit\u00e9s int\u00e9gr\u00e9es pour le d\u00e9veloppement web. Il est id\u00e9al pour les applications web complexes.  </p> </li> <li> <p>Benchmark : Flask, connu pour sa l\u00e9g\u00e8ret\u00e9 et sa flexibilit\u00e9 exceptionnelle, a \u00e9t\u00e9 choisi pour notre projet. Bien que Django offre une multitude de fonctionnalit\u00e9s int\u00e9gr\u00e9es qui am\u00e9liorent la productivit\u00e9, l'architecture l\u00e9g\u00e8re et flexible de Flask correspondait davantage \u00e0 nos besoins sp\u00e9cifiques.</p> </li> </ul>"},{"location":"Deployment/#2-reactjs-vs-angular-vs-vuejs-pour-le-frontend","title":"2. React.js vs. Angular vs. Vue.js (pour le frontend)","text":"<ul> <li> <p>React.js : React sont des biblioth\u00e8ques JavaScript pour la cr\u00e9ation d'interfaces utilisateur interactives. Elles se concentrent sur la cr\u00e9ation de composants r\u00e9utilisables.  </p> </li> <li> <p>Angular : Angular est un framework JavaScript complet pour le d\u00e9veloppement d'applications web. Il fournit une structure plus stricte et des fonctionnalit\u00e9s compl\u00e8tes.    </p> </li> <li> <p>Vue.js : Vue est un framework JavaScript progressif qui se situe entre React et Angular en termes de complexit\u00e9.  </p> </li> <li> <p>Benchmark: En ce qui concerne React, il est largement adopt\u00e9 pour son approche conviviale et la robustesse de sa communaut\u00e9 de d\u00e9veloppeurs. En revanche, Angular convient mieux aux applications d'entreprise complexes. Vue constitue un choix interm\u00e9diaire, offrant convivialit\u00e9 pour les projets de petite \u00e0 moyenne envergure. \u00c9tant donn\u00e9 la pr\u00e9valence de la popularit\u00e9 de React.js, il pr\u00e9sente l'avantage de disposer de solutions facilement disponibles pour r\u00e9soudre les d\u00e9fis de codage, gr\u00e2ce \u00e0 son vaste support communautaire.  </p> </li> </ul>"},{"location":"Deployment/#3-vtkjs-vs-threejs-pour-la-visualisation-3d","title":"3. VTK.js vs. Three.js (pour la visualisation 3D) :","text":"<ul> <li> <p>VTK.js : VTK.js est une biblioth\u00e8que JavaScript bas\u00e9e sur le Visualization Toolkit (VTK) pour la visualisation 3D dans le navigateur.   </p> </li> <li> <p>Three.js : Three.js est une biblioth\u00e8que JavaScript populaire pour la cr\u00e9ation d'applications de visualisation 3D en temps r\u00e9el.    </p> </li> <li> <p>Benchmark: Le choix entre VTK.js et Three.js d\u00e9pend des caract\u00e9ristiques particuli\u00e8res de votre application. VTK.js convient bien lorsque des capacit\u00e9s avanc\u00e9es de traitement de donn\u00e9es scientifiques et m\u00e9dicales sont n\u00e9cessaires, tandis que Three.js offre une plus grande polyvalence pour des visualisations 3D g\u00e9n\u00e9rales. \u00c9tant donn\u00e9 que notre projet implique une application m\u00e9dicale avec des besoins sp\u00e9cifiques de traitement des donn\u00e9es, notre choix s'est port\u00e9 sur VTK.js.</p> </li> </ul>"},{"location":"Deployment/#visualisation-3d-avec-vtkjs","title":"visualisation 3D avec VTK.js","text":""},{"location":"Deployment/#1-qui-ce-que-cest-rt-quelle-utilite","title":"1. Qui ce que c'est rt quelle utilit\u00e9?","text":"<p>VTK.js, ou Visualization Toolkit for JavaScript, est une biblioth\u00e8que open source tr\u00e8s puissante et polyvalente con\u00e7ue pour la visualisation 3D dans les applications web. </p> <ul> <li>Large \u00c9cosyst\u00e8me : VTK.js s'appuie sur le c\u00e9l\u00e8bre kit de visualisation VTK (Visualization Toolkit) utilis\u00e9 dans de nombreuses applications scientifiques et industrielles. Cela signifie qu'il b\u00e9n\u00e9ficie de nombreuses ann\u00e9es de d\u00e9veloppement, de raffinement et d'une grande communaut\u00e9 d'utilisateurs.  </li> <li>Support Multiplateforme : VTK.js est con\u00e7u pour \u00eatre compatible avec une grande vari\u00e9t\u00e9 de navigateurs web modernes, ce qui facilite l'acc\u00e8s aux visualisations 3D sur diff\u00e9rentes plates-formes, y compris les ordinateurs de bureau, les tablettes et les appareils mobiles.  </li> <li>Personnalisable : L'une des forces de VTK.js r\u00e9side dans sa capacit\u00e9 \u00e0 \u00eatre hautement personnalisable. Vous pouvez cr\u00e9er des visualisations sur mesure en utilisant des composants VTK.js, en ajustant l'apparence, en ajoutant des interactions utilisateur et en adaptant les fonctionnalit\u00e9s aux besoins sp\u00e9cifiques de votre application.  </li> <li>Interop\u00e9rabilit\u00e9 : VTK.js offre des fonctionnalit\u00e9s d'interop\u00e9rabilit\u00e9 avec d'autres biblioth\u00e8ques et technologies web couramment utilis\u00e9es, telles que Three.js et WebGL, ce qui permet d'int\u00e9grer facilement des composants 3D dans des applications existantes.  </li> <li>Utilisation dans Divers Domaines : VTK.js est largement utilis\u00e9 dans divers domaines tels que la m\u00e9decine, la recherche scientifique, l'ing\u00e9nierie, la g\u00e9oscience, la visualisation de donn\u00e9es et la simulation. Il est particuli\u00e8rement pr\u00e9cieux pour afficher des mod\u00e8les anatomiques, des simulations num\u00e9riques, des cartes g\u00e9ographiques et des mod\u00e8les architecturaux.  </li> <li>Documentation Abondante : VTK.js b\u00e9n\u00e9ficie d'une documentation d\u00e9taill\u00e9e et de nombreux exemples, ce qui facilite l'apprentissage et le d\u00e9veloppement d'applications de visualisation 3D.  </li> <li>\u00c9volutif : La biblioth\u00e8que est en constante \u00e9volution, avec des mises \u00e0 jour r\u00e9guli\u00e8res qui ajoutent de nouvelles fonctionnalit\u00e9s et am\u00e9liorent les performances.  </li> </ul> <p>En r\u00e9sum\u00e9, VTK.js est un outil pr\u00e9cieux pour quiconque souhaite int\u00e9grer des visualisations 3D interactives dans des applications web, offrant une flexibilit\u00e9, une puissance et une compatibilit\u00e9 multiplateforme in\u00e9gal\u00e9es.</p>"},{"location":"Deployment/#2-quels-avantages","title":"2. Quels avantages ?","text":"<p>La visualisation 3D avec VTK.js offre une multitude d'avantages captivants pour les professionnels de divers domaines. Tout d'abord, elle permet une repr\u00e9sentation tridimensionnelle immersive des donn\u00e9es, ce qui facilite la compr\u00e9hension des structures complexes et des ph\u00e9nom\u00e8nes spatiaux. Ensuite, VTK.js offre une flexibilit\u00e9 remarquable en permettant la visualisation de donn\u00e9es provenant de diverses sources, que ce soit des simulations scientifiques, des scans m\u00e9dicaux, ou des mod\u00e8les architecturaux. De plus, gr\u00e2ce \u00e0 sa compatibilit\u00e9 avec les navigateurs web, VTK.js rend la visualisation 3D accessible \u00e0 un large public, sans n\u00e9cessiter l'installation de logiciels sp\u00e9cifiques. Enfin, la possibilit\u00e9 d'interagir en temps r\u00e9el avec les visualisations permet une exploration approfondie des donn\u00e9es, favorisant ainsi la prise de d\u00e9cision \u00e9clair\u00e9e dans des domaines tels que la recherche, la m\u00e9decine, l'ing\u00e9nierie, et bien d'autres encore. En somme, VTK.js r\u00e9volutionne la mani\u00e8re dont nous analysons et comprenons les donn\u00e9es en offrant une exp\u00e9rience de visualisation 3D immersive, accessible et flexible.</p>"},{"location":"FAQ/","title":"FAQ probl\u00e8mes-solutions","text":""},{"location":"FAQ/#probleme-1-visualisation-de-fichiers-3d","title":"Probl\u00e8me 1 : Visualisation de fichiers 3D","text":"<p>Q: Comment pouvons-nous visualiser des fichiers 3D dans notre projet MeshSegNet en utilisant la biblioth\u00e8que VTK.js ?</p> <p>R: Nous pouvons utiliser la biblioth\u00e8que VTK.js pour la visualisation de fichiers 3D. Assurez-vous d'int\u00e9grer correctement VTK.js dans votre projet et de consulter la documentation de VTK.js pour apprendre \u00e0 l'utiliser. Documentation vtkjs</p>"},{"location":"FAQ/#probleme-2-gestion-des-gpu-dans-google-colab-et-kaggle","title":"Probl\u00e8me 2 : Gestion des GPU dans Google Colab et Kaggle","text":"<p>Q: Nous avons des probl\u00e8mes avec l'utilisation de GPU dans Google Colab, et nous souhaitons \u00e9galement utiliser Kaggle. Comment pouvons-nous r\u00e9soudre cela  ?</p> <p>R: Nous pouvons sp\u00e9cifier l'utilisation d'un GPU dans Google Colab en allant dans \"Modifier\" -&gt; \"Param\u00e8tres du Notebook\" -&gt; \"Mat\u00e9riel d'ex\u00e9cution\". Pour Kaggle, allez dans les param\u00e8tres du Kernel et s\u00e9lectionnez \"Acc\u00e9l\u00e9rateur GPU\". Assurez-vous d'importer les biblioth\u00e8ques GPU telles que TensorFlow ou PyTorch pour utiliser efficacement la puissance du GPU.</p>"},{"location":"FAQ/#probleme-3-conversion-de-fichiers-de-donnees-blend-to-obj-to-vtp","title":"Probl\u00e8me 3 : Conversion de fichiers de donn\u00e9es (.blend to .obj to .vtp)","text":"<p>Q: Notre jeu de donn\u00e9es est au format .blend, mais nous avons besoin de le convertir en .obj ou .vtp pour l'entra\u00eenement. Comment pouvons-nous faire cela  ?</p> <p>R: Nous pouvons utiliser Blender pour convertir nos fichiers .blend en .obj. Suivez ces \u00e9tapes : 1. Ouvrez Blender. 2. Chargez votre fichier .blend. 3. S\u00e9lectionnez l'objet que vous souhaitez exporter. 4. Allez dans \"Fichier\" -&gt; \"Exporter\" -&gt; \"Wavefront (.obj)\" et suivez les instructions pour exporter le fichier.</p> <p>Pour convertir en .vtp, nous pouvons utiliser la biblioth\u00e8que <code>pyvista</code> en Python pour g\u00e9rer les fichiers VTK. Assurez-vous de l'installer via <code>pip</code> ou <code>conda</code>.</p> <p>N'oublions pas d'adapter ces solutions \u00e0 notre propre environnement de projet et aux besoins sp\u00e9cifiques de notre mod\u00e8le MeshSegNet.</p>"},{"location":"about/","title":"About","text":""},{"location":"about/#a-propos-de-lequipe","title":"\u00c0 propos de l'\u00e9quipe","text":"<p>Bienvenue dans la documentation de Project-MeshSegNet. Voici l'\u00e9quipe qui a contribu\u00e9 \u00e0 ce projet :</p>"},{"location":"about/#agdad-mariam","title":"Agdad Mariam","text":"<p>Domaine: G\u00e9nie Informatique Email :mariam.agdad08@gmail.com linkedin github</p>"},{"location":"about/#elhassnaoui-mohamed","title":"Elhassnaoui Mohamed","text":"<p>Domaine: Science des donn\u00e9es Email :mohamed2001elhassnaoui@gmail.com linkedin github</p>"},{"location":"about/#elmouwahid-ayoub","title":"Elmouwahid Ayoub","text":"<p>Domaine: Ing\u00e9nierie des Syst\u00e8mes d'Information et de Communication Email :elmouwahid2001@gmail.com linkedin github</p>"},{"location":"about/#labrouki-yousra","title":"Labrouki Yousra","text":"<p>Domaine: G\u00e9nie Logiciel Email :yousralabrouki@gmail.com linkedin github</p>"},{"location":"about/#noutfi-fatima","title":"Noutfi Fatima","text":"<p>Domaine: Intelligence Artificielle Email :fatima_noutfi@um5.ac.ma linkedin github</p>"},{"location":"about/#touzouz-adnane","title":"Touzouz Adnane","text":"<p>Domaine: Inteligence Artificiele et Analyse des Donn\u00e9es Email :touzouzadnane0@gmail.com linkedin github</p>"}]}